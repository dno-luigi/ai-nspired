// OpenRouter Universal Preset Proxy with Unlimited Context
export default {
  async fetch(request, env) {
    if (request.method === 'OPTIONS') {
      return new Response(null, {
        headers: {
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type, Authorization',
        }
      });
    }

    if (request.method !== 'POST') {
      return new Response('Method not allowed', { status: 405 });
    }

    try {
      const { prompt, preset, stream = false, tools = [], context_id = null } = await request.json();
      
      if (!prompt) {
        return new Response(JSON.stringify({ error: 'Prompt is required' }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        });
      }

      const handler = new UniversalPresetHandler(env);
      
      if (stream) {
        return handler.handleStreamingRequest(prompt, preset, tools, context_id);
      } else {
        return handler.handleRequest(prompt, preset, tools, context_id);
      }
    } catch (error) {
      return new Response(JSON.stringify({ 
        error: 'Internal server error', 
        details: error.message 
      }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      });
    }
  }
};

class UniversalPresetHandler {
  constructor(env) {
    this.env = env;
    this.openRouterKey = env.OPENROUTER_API_KEY;
    this.baseUrl = 'https://openrouter.ai/api/v1';
  }

  // Unlimited Context System with Middle-Out Transform
  async buildUnlimitedContext(prompt, contextId) {
    if (!contextId) return [];

    try {
      // Get embedding for current prompt
      const promptEmbedding = await this.getEmbedding(prompt);
      
      // Retrieve stored context chunks
      const storedChunks = await this.getStoredContext(contextId);
      
      if (storedChunks.length === 0) return [];

      // Calculate relevance scores using cosine similarity
      const rankedChunks = storedChunks.map(chunk => ({
        ...chunk,
        relevance: this.cosineSimilarity(promptEmbedding, chunk.embedding)
      })).sort((a, b) => b.relevance - a.relevance);

      // Middle-out transform: select most relevant chunks from different time periods
      const contextMessages = this.middleOutTransform(rankedChunks, 8000); // ~8k token budget
      
      return contextMessages;
    } catch (error) {
      console.error('Context retrieval error:', error);
      return [];
    }
  }

  // Middle-Out Transform: Select chunks from beginning, end, and most relevant middle
  middleOutTransform(rankedChunks, tokenBudget) {
    if (rankedChunks.length === 0) return [];

    const result = [];
    let usedTokens = 0;
    const avgTokensPerChunk = 200; // Estimate

    // 1. Always include the most recent chunks (end)
    const recentChunks = rankedChunks.slice(0, 3);
    for (const chunk of recentChunks) {
      if (usedTokens + avgTokensPerChunk > tokenBudget) break;
      result.push(chunk);
      usedTokens += avgTokensPerChunk;
    }

    // 2. Include oldest relevant chunks (beginning)
    const oldestChunks = rankedChunks.slice(-2);
    for (const chunk of oldestChunks) {
      if (usedTokens + avgTokensPerChunk > tokenBudget) break;
      if (!result.find(r => r.id === chunk.id)) {
        result.push(chunk);
        usedTokens += avgTokensPerChunk;
      }
    }

    // 3. Fill remaining budget with most relevant middle chunks
    const middleChunks = rankedChunks.slice(3, -2);
    for (const chunk of middleChunks) {
      if (usedTokens + avgTokensPerChunk > tokenBudget) break;
      if (!result.find(r => r.id === chunk.id)) {
        result.push(chunk);
        usedTokens += avgTokensPerChunk;
      }
    }

    // Sort by timestamp for coherent conversation flow
    return result.sort((a, b) => a.timestamp - b.timestamp).map(chunk => ({
      role: chunk.role,
      content: chunk.content
    }));
  }

  async getEmbedding(text) {
    const response = await fetch(`${this.baseUrl}/embeddings`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openRouterKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://your-domain.com',
        'X-Title': 'Universal Preset Proxy'
      },
      body: JSON.stringify({
        model: 'text-embedding-3-small', // Fast and cost-effective
        input: text
      })
    });

    if (!response.ok) {
      throw new Error(`Embedding API error: ${response.status}`);
    }

    const data = await response.json();
    return data.data[0].embedding;
  }

  async getStoredContext(contextId) {
    // Retrieve from KV storage
    const stored = await this.env.CONTEXT_STORE.get(`context:${contextId}`);
    return stored ? JSON.parse(stored) : [];
  }

  async storeContext(contextId, messages) {
    // Store with embeddings for future retrieval
    const chunksWithEmbeddings = [];
    
    for (const message of messages) {
      if (message.content && message.content.length > 50) { // Only embed substantial content
        try {
          const embedding = await this.getEmbedding(message.content);
          chunksWithEmbeddings.push({
            id: `${Date.now()}-${Math.random()}`,
            role: message.role,
            content: message.content,
            embedding: embedding,
            timestamp: Date.now()
          });
        } catch (error) {
          console.error('Failed to embed message:', error);
          // Store without embedding as fallback
          chunksWithEmbeddings.push({
            id: `${Date.now()}-${Math.random()}`,
            role: message.role,
            content: message.content,
            embedding: null,
            timestamp: Date.now()
          });
        }
        
        
      }
    }

    // Merge with existing context
    const existing = await this.getStoredContext(contextId);
    const merged = [...existing, ...chunksWithEmbeddings];
    
    // Keep only last 1000 chunks to prevent unlimited growth
    const trimmed = merged.slice(-1000);
    
    await this.env.CONTEXT_STORE.put(`context:${contextId}`, JSON.stringify(trimmed));
  }

  cosineSimilarity(a, b) {
    if (!a || !b || a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  // Handle streaming requests with interleaved thinking
  async handleStreamingRequest(prompt, preset, tools, contextId) {
    const contextMessages = await this.buildUnlimitedContext(prompt, contextId);
    
    const messages = [
      ...contextMessages,
      { role: 'user', content: prompt }
    ];

    const requestBody = {
      model: preset ? `@preset/${preset}` : 'anthropic/claude-sonnet-4.5',
      messages: messages,
      stream: true,
      tools: tools.length > 0 ? tools : undefined,
      // Enable thinking for supported models
      extra_body: {
        thinking: true
      }
    };

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openRouterKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://your-domain.com',
        'X-Title': 'Universal Preset Proxy'
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      throw new Error(`OpenRouter API error: ${response.status}`);
    }

    // Create a transform stream to handle interleaved thinking
    const { readable, writable } = new TransformStream({
      transform: async (chunk, controller) => {
        const decoder = new TextDecoder();
        const text = decoder.decode(chunk);
        
        // Parse SSE chunks and handle thinking blocks
        const lines = text.split('\n');
        for (const line of lines) {
          if (line.startsWith('data: ') && line !== 'data: [DONE]') {
            try {
              const data = JSON.parse(line.slice(6));
              
              // Handle thinking blocks specially
              if (data.choices?.[0]?.delta?.thinking) {
                controller.enqueue(new TextEncoder().encode(
                  `data: ${JSON.stringify({
                    type: 'thinking',
                    content: data.choices[0].delta.thinking
                  })}\n\n`
                ));
              }
              
              // Handle regular content
              if (data.choices?.[0]?.delta?.content) {
                controller.enqueue(new TextEncoder().encode(
                  `data: ${JSON.stringify({
                    type: 'content',
                    content: data.choices[0].delta.content
                  })}\n\n`
                ));
              }
              
              // Handle tool calls
              if (data.choices?.[0]?.delta?.tool_calls) {
                controller.enqueue(new TextEncoder().encode(
                  `data: ${JSON.stringify({
                    type: 'tool_call',
                    tool_calls: data.choices[0].delta.tool_calls
                  })}\n\n`
                ));
              }
            } catch (e) {
              // Forward malformed chunks as-is
              controller.enqueue(chunk);
            }
          } else {
            controller.enqueue(new TextEncoder().encode(line + '\n'));
          }
        }
      }
    });

    // Store the conversation for future context
    if (contextId) {
      // We'll store this after the stream completes
      // For now, just store the user message
      await this.storeContext(contextId, [{ role: 'user', content: prompt }]);
    }

    // Pipe the response through our transform
    response.body.pipeTo(writable);

    return new Response(readable, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization',
      }
    });
  }

  // Handle non-streaming requests
  async handleRequest(prompt, preset, tools, contextId) {
    const contextMessages = await this.buildUnlimitedContext(prompt, contextId);
    
    const messages = [
      ...contextMessages,
      { role: 'user', content: prompt }
    ];

    const requestBody = {
      model: preset ? `@preset/${preset}` : 'anthropic/claude-sonnet-4.5',
      messages: messages,
      tools: tools.length > 0 ? tools : undefined,
      extra_body: {
        thinking: true
      }
    };

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openRouterKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://your-domain.com',
        'X-Title': 'Universal Preset Proxy'
      },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`OpenRouter API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    
    // Store the conversation for future context
    if (contextId) {
      const conversationMessages = [
        { role: 'user', content: prompt },
        { role: 'assistant', content: data.choices[0].message.content }
      ];
      await this.storeContext(contextId, conversationMessages);
    }

    return new Response(JSON.stringify({
      response: data.choices[0].message.content,
      thinking: data.choices[0].message.thinking || null,
      model: data.model,
      usage: data.usage,
      tool_calls: data.choices[0].message.tool_calls || null,
      context_used: contextMessages.length > 0
    }), {
      headers: {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization',
      }
    });
  }
}
